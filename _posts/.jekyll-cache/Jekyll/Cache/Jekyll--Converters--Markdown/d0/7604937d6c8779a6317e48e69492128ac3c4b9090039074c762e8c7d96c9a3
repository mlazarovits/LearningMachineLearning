I"W<h2 id="regression">Regression</h2>
<p>We learned two models for regression. A simple linear regression as an example of a parametric method, and Gaussian Process as an example for the non-parametric method.</p>

<h2 id="linear-regression">Linear Regression</h2>
<p>In simple linear regression, we determine the parameters (ğ‘¤, Ïµ) by fitting a linear equation, y=ğ‘¤x+Ïµ to the observed data. The figure of merit is the mean squared error on the predicted model y^=ğ‘¤(x*)+Ïµ, where is unseen points of interest. We did two examples, where in first example we use all data for regression, and then test it on any other dataset point x*[1,â€¦, 20]. In second example, we use half of the data for training, i.e. to determine the parameters (ğ‘¤, Ïµ) of the linear model, and use the rest of the data to test the fitted line. In both examples, we plot residuals or mean squared error.</p>

<h2 id="gaussian-process-regression">Gaussian Process Regression</h2>
<p>Bayesâ€™ Rule allow us to infer about the posterior distribution 
ğ‘(ğ‘¤|ğ‘¦,ğ‘‹)
by specifying a prior distribution, ğ‘(ğ‘¤), on the parameter, ğ‘¤, and relocating probabilities based on evidence (i.e. observed data).</p>

<p>ğ‘(ğ‘¤|ğ‘¦,ğ‘‹)=ğ‘(ğ‘Œ|ğ‘‹,ğ‘¤)ğ‘(ğ‘¤) / ğ‘(ğ‘Œ|ğ‘‹)</p>

<p>The updated distribution ğ‘(ğ‘¤|ğ‘¦,ğ‘‹), called the posterior distribution, thus incorporates information from both the prior distribution and the dataset. To get predictions at unseen points of interest, x*, the predictive distribution can be calculated by weighting all possible predictions by their calculated posterior distribution.</p>

<p>ğ‘(ğ‘“âˆ—|ğ‘¥âˆ—,ğ‘¦,ğ‘‹) = âˆ«ğ‘¤ğ‘(ğ‘“âˆ—|ğ‘¥âˆ—,ğ‘¤) ğ‘(ğ‘¤|ğ‘¦,ğ‘‹) ğ‘‘ğ‘¤</p>

<p>ğ‘(ğ‘“âˆ—|ğ‘¥âˆ—,ğ‘¦,ğ‘‹) = N(ğ‘“âˆ—|ğœ‡âˆ—,Î£âˆ—)</p>

<p>The prior and likelihood is usually assumed to be Gaussian for the integration to be tractable, and thus the predictive distribution, is also a Gaussian distribution, from which we can obtain a point prediction, using its mean (ğœ‡âˆ—) and an uncertainty quantification using its variance (Î£âˆ—). Therefore, instead of calculating the probability distribution of parameters of a specific function, GP calculates the probability distribution over all admissible functions that fit the data.</p>

<p>In GP regression, we first assume a Gaussian process prior, which can be specified using a mean function, m(x), and covariance function, k(x, xâ€™):</p>

<p>ğ‘“(ğ‘¥) ~ GP ( m(X), k(x, xâ€™) )</p>

<p>A Gaussian process is like an infinite-dimensional multivariate Gaussian distribution, where any collection of the labels, y(x) of the dataset has a joint Gaussian distribution. We can also incorporate independently, identically distributed Gaussian noise, Ïµ âˆ¼ N(0, ÏƒÂ²) to the labels.</p>

<p>y(x) = ğ‘“(ğ‘¥) + Ïµ</p>

<p>y(x) ~ GP ( m(X), k(x,xâ€™) + Î´ij ÏƒÂ²y)</p>

<p>Since the collection of training points and test points are joint multivariate Gaussian distributed, and so we can write their distribution as follows:</p>

<p>From this, we can calculate ğœ‡âˆ— and Î£âˆ—, which are the mean and variance of the predictive posterior distribution.</p>

<p>In third tutorial, we draw samples from a prior distribution, with a mean and a covariance to visualize the prior distribution. Then we compute the predictive posterior distribution assuming zero noise in training data. We study the effect of Kernel parameters and noise effect on the posterior predictive distributions. In last step, we found the optimal parameters of the Kernel, and the noise by maximizing the marginal log-likelihood, and then plot the predictive posterior distribution.</p>

:ET