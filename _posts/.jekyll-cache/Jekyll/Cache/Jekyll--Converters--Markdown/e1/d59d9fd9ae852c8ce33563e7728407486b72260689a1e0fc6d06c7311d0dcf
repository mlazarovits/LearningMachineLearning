I"\<p>This week, John and Quinn lead a discussion on <a href="../../../assets/papers_summer19/GANs_1406.261.pdf">Generative Adversarial Networks (GANs)</a>, networks consisting of two opposing subnetworks that train against each other. A generator network tries to map vectors from some noise distribution to the data distribution convincingly, while a discriminator network learns to accurately differentiate between actual data and data generated by the generator network. This corresponds to the two networks playing a ‘minimax game’ that has a global minimum when the generator completely recreates the data distribution using its map from the noise distribution. The discriminator and generator are trained simultaneously until they reach a suitably accurate performance.</p>

<p>GANs have multiple advantages over traditional networks when it comes to computational efficiency: they do not require Markov chains for sampling, they only calculate gradients using backpropagation, and they do not make any inference during the learning period. Additionally, the final model created by a GAN is rather flexible in its form: it need only be differentiable, in contrast to more traditional network models that often provide actual probability distributions with learned parameters. This is because the model only consists of a function that, when sampled, creates samples similar to that of the actual data distribution.</p>

<p>For more information, please check the slides <a href="../../../assets/pwrpts_summer19/GAN Presentation.pdf">here</a>.</p>

<p>Thank you to John Pace for this summary on this week’s meeting.</p>
:ET